function valueIteration(ğ–²::Int, ğ– ::Int, dataset::DataFrame, reachableStates,
    Î³::Float64, Ïµ::Float64)

    T, R = inferTransitionAndReward(dataset, ğ–², ğ– )

    # compute and show the Bellman residual
    Î´ = Ïµ*(1-Î³)/Î³
    bellmanResidual = Î´+1
    @show Î´

    U  = zeros(ğ–²)
    Up = zeros(ğ–²)
    Ï€  = zeros(ğ–²)

    sumOfDiscountedFutureRewards = zeros(ğ–², ğ– )
    immediateReward              = zeros(ğ–², ğ– )

    # initialize immediate reward matrix
    for s in 1:ğ–², a in 1:ğ– 
        immediateReward[s, a] = get(R, (s, a), 0)
    end

    k = 1

    while bellmanResidual > Î´

        sumOfDiscountedFutureRewards = zeros(ğ–², ğ– )

        for s in 1:ğ–², a in 1:ğ– 

            sumOfDiscountedFutureRewards[s, a] = Î³*sum(get(T, (s, a, sp), 0)*Up[sp] for sp in reachableStates(s))

        end

        # update value function and policy over the entire state space
        Up, Ï€ = findmax(immediateReward + sumOfDiscountedFutureRewards, dims=2)
        bellmanResidual = maximum(abs.(Up - U))

        @show k
        @show bellmanResidual

        k += 1
        U = deepcopy(Up)

    end

    return Up, collect(Ï€[i][2] for i in 1:ğ–²)

end
