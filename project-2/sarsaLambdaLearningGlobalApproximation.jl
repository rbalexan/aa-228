function sarsaLambdaLearningGlobalApproximation(ğ–²::Int, ğ– ::Int, dataset::DataFrame, Î²,
    Î±::Float64=0.9, Î³::Float64=1.0, Î»::Float64=0.9)

    b = size(Î²(1,2))[1]
    # initialize Î¸ and N
    Î¸ = zeros(b)
    N = zeros(ğ–², ğ– )
    Q = zeros(ğ–², ğ– )
    A = zeros(ğ–²*ğ– , b)
    B = zeros(ğ–², ğ– , b)

    # Hankel matrix (for second-order polynomial) and its singular value decomposition
    for i in 1:ğ–²*ğ– 
        A[i, :] = Î²(mod(i,ğ–²), ceil(i/ğ–²))
    end

    for s in 1:ğ–², a in 1:ğ– 
        B[s, a, :] = Î²(s, a)
    end


    F = svd(A)
    W = F.V*inv(Diagonal(F.S))*F.U'

    # loop over the dataset
    for i in 1:size(dataset)[1]

        # if we are at the end of an episode, reset the counts for next episode
        # and skip the last sarsa iteration
        if i == size(dataset)[1] || dataset.sp[i] â‰  dataset.s[i+1]
            N = zeros(ğ–², ğ– )
            continue
        end

        s   = dataset.s[i]
        a   = dataset.a[i]
        r   = dataset.r[i]
        sp  = dataset.s[i+1]
        ap  = dataset.a[i+1]

        N[s, a] += 1
        Î´       =  r + Î³*sum(Î¸.*B[sp, ap, :]) - sum(Î¸.*B[s, a, :])

        for s in 1:ğ–², a in 1:ğ– 

            Q[s, a] += Î±*Î´*N[s, a]
            N[s, a] *= Î³*Î»

        end

        # predicted model
        Î¸ = W*reshape(Q, 400, :)

    end

    Q = zeros(ğ–², ğ– )
    for s in 1:ğ–², a in 1:ğ– 
        Q[s, a] = sum(Î¸.*B[s, a, :])
    end

    Ï€Ind = argmax(Q, dims=2)

    U = Q[Ï€Ind]
    Ï€ = collect(Ï€Ind[i][2] for i in 1:ğ–²)

    return U, Ï€

end
