function solveMDP(p::Problem)

    # Initialize state
    v = p.V
    t = 0

    # Initialize Q
    ğ–² = p.V*p.T
    ğ–  = prod([length(p.F[f][7]) for f in 1:length(p.F)])
    Q = zeros(ğ–², ğ– )

    # Initialize reward
    r = 0

    s_index    = LinearIndices((1:p.V, 1:p.T))[v, t] # may need to change the linear indexing
    Ïµ_gaussian = rand(Normal(p.Ïµ, 0), 1)[]
    a = rand(Bernoulli(Ïµ_gaussian), 1)[] == 1 ? rand(1:300, 1) : argmax(Q[s_index, :])

    # Loop along time
    # *Code to be added*\

    # Extract policy
    Ï€â‹† = argmax(Q, dims=2) # can replace with argmax(Q, dim=1), I think
    Ï€â‹† = [Ï€â‹†[s][2] for s in 1:ğ–²]
end
